"""
ç½‘é¡µé˜…è¯»å·¥å…·
è¯»å–å’Œè§£æžç½‘é¡µå†…å®¹
"""

import httpx
from bs4 import BeautifulSoup
from markdownify import markdownify
from langchain_core.tools import BaseTool
from pydantic import Field


class ReaderTool(BaseTool):
    """ç½‘é¡µé˜…è¯»å·¥å…·"""

    name: str = "read_url"
    description: str = """é˜…è¯»æŒ‡å®š URL çš„ç½‘é¡µå†…å®¹ã€‚
å½“ä½ éœ€è¦æ·±å…¥é˜…è¯»æŸç¯‡æ–‡ç« ã€è®ºæ–‡æˆ–ç½‘é¡µæ—¶ä½¿ç”¨æ­¤å·¥å…·ã€‚
è¾“å…¥åº”è¯¥æ˜¯å®Œæ•´çš„ URL åœ°å€ã€‚"""

    max_length: int = Field(default=3000)
    timeout: int = Field(default=10)

    def _run(self, url: str) -> str:
        """è¯»å–ç½‘é¡µ"""
        try:
            # è¯·æ±‚ç½‘é¡µ
            headers = {
                "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
                "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
            }

            with httpx.Client(timeout=self.timeout, follow_redirects=True) as client:
                response = client.get(url, headers=headers)
                response.raise_for_status()

            # è§£æž HTML
            soup = BeautifulSoup(response.text, "html.parser")

            # ç§»é™¤ä¸éœ€è¦çš„å…ƒç´ 
            for tag in soup(["script", "style", "nav", "footer", "header", "aside"]):
                tag.decompose()

            # æå–æ ‡é¢˜
            title = soup.find("title")
            title_text = title.get_text().strip() if title else "æ— æ ‡é¢˜"

            # æå–æ­£æ–‡
            # å°è¯•å¸¸è§çš„æ–‡ç« å®¹å™¨
            article = (
                soup.find("article")
                or soup.find("main")
                or soup.find("div", class_="content")
                or soup.find("body")
            )

            if article:
                # è½¬æ¢ä¸º Markdown
                content = markdownify(str(article), heading_style="ATX")
            else:
                content = soup.get_text()

            # æ¸…ç†å’Œæˆªæ–­
            content = self._clean_text(content)
            if len(content) > self.max_length:
                content = content[: self.max_length] + "\n\n...[å†…å®¹å·²æˆªæ–­]"

            return f"ðŸ“„ {title_text}\næ¥æº: {url}\n\n{content}"

        except httpx.TimeoutException:
            return f"è¯»å–è¶…æ—¶: {url}"
        except httpx.HTTPError as e:
            return f"HTTP é”™è¯¯: {str(e)}"
        except Exception as e:
            return f"è¯»å–å¤±è´¥: {str(e)}"

    def _clean_text(self, text: str) -> str:
        """æ¸…ç†æ–‡æœ¬"""
        import re

        # ç§»é™¤å¤šä½™ç©ºè¡Œ
        text = re.sub(r"\n{3,}", "\n\n", text)
        # ç§»é™¤å¤šä½™ç©ºæ ¼
        text = re.sub(r" {2,}", " ", text)
        return text.strip()

    async def _arun(self, url: str) -> str:
        """å¼‚æ­¥æ‰§è¡Œ"""
        return self._run(url)


def create_reader_tool(max_length: int = 3000) -> ReaderTool:
    """åˆ›å»ºé˜…è¯»å·¥å…·å®žä¾‹"""
    return ReaderTool(max_length=max_length)
