[
  {
    "id": 1,
    "title": "Transformer 架构",
    "content": "Transformer 是一种用于 NLP 任务的架构，由 Encoder 和 Decoder 组成。Encoder 负责将输入序列转换为编码矩阵，Decoder 负责根据编码矩阵生成输出序列。Transformer 的核心是 Multi-Head Attention 机制，它由多个 Self-Attention 组成，用于捕捉输入序列中单词之间的关系。Transformer 使用单词 Embedding 和位置 Embedding 来表示输入序列中的单词。",
    "tags": [],
    "created_at": "2026-02-23T02:06:10.860849"
  },
  {
    "id": 2,
    "title": "BERT 和 GPT",
    "content": "BERT 类似于 Transformer 的 Encoder 部分，主要用于自然语言理解任务。GPT 类似于 Transformer 的 Decoder 部分，主要用于自然语言生成任务。BERT 使用双向预训练，而 GPT 使用自回归预训练。",
    "tags": [],
    "created_at": "2026-02-23T02:06:15.062986"
  }
]