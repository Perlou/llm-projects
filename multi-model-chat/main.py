#!/usr/bin/env python3
"""
å¤šæ¨¡å‹å¯¹æ¯”èŠå¤©åº”ç”¨

Usage:
    python main.py                          # äº¤äº’æ¨¡å¼
    python main.py --query "ä½ çš„é—®é¢˜"       # å•æ¬¡æŸ¥è¯¢
    python main.py --models gpt-4 claude-3-sonnet --query "ä½ å¥½"
"""

import asyncio
import argparse
import sys
from typing import Optional

from config import MODELS, get_api_key, get_default_models, get_ollama_host
from models import (
    BaseModel,
    ChatMessage,
    ChatResponse,
    OpenAIModel,
    ClaudeModel,
    GeminiModel,
    OllamaModel,
)
from ui import (
    print_welcome,
    print_model_list,
    print_user_input,
    print_responses,
    print_comparison_table,
    get_user_input,
    print_goodbye,
    print_error,
    print_info,
)


def create_model(model_key: str) -> Optional[BaseModel]:
    """æ ¹æ®æ¨¡å‹ key åˆ›å»ºæ¨¡å‹å®ä¾‹"""
    if model_key not in MODELS:
        print_error(f"æœªçŸ¥æ¨¡å‹: {model_key}")
        return None

    config = MODELS[model_key]
    provider = config.provider

    # è·å– API Key
    api_key = get_api_key(provider)

    # æ ¹æ®æä¾›å•†åˆ›å»ºæ¨¡å‹
    if provider == "openai":
        if not api_key:
            print_info(f"è·³è¿‡ {model_key}: æœªé…ç½® OPENAI_API_KEY")
            return None
        return OpenAIModel(
            api_key=api_key,
            name=config.name,
            provider=provider,
            model_id=config.model_id,
            max_tokens=config.max_tokens,
            temperature=config.temperature,
            input_price=config.input_price,
            output_price=config.output_price,
        )

    elif provider == "anthropic":
        if not api_key:
            print_info(f"è·³è¿‡ {model_key}: æœªé…ç½® ANTHROPIC_API_KEY")
            return None
        return ClaudeModel(
            api_key=api_key,
            name=config.name,
            provider=provider,
            model_id=config.model_id,
            max_tokens=config.max_tokens,
            temperature=config.temperature,
            input_price=config.input_price,
            output_price=config.output_price,
        )

    elif provider == "gemini":
        if not api_key:
            print_info(f"è·³è¿‡ {model_key}: æœªé…ç½® GOOGLE_API_KEY")
            return None
        return GeminiModel(
            api_key=api_key,
            name=config.name,
            provider=provider,
            model_id=config.model_id,
            max_tokens=config.max_tokens,
            temperature=config.temperature,
            input_price=config.input_price,
            output_price=config.output_price,
        )

    elif provider == "ollama":
        return OllamaModel(
            host=get_ollama_host(),
            name=config.name,
            provider=provider,
            model_id=config.model_id,
            max_tokens=config.max_tokens,
            temperature=config.temperature,
            input_price=config.input_price,
            output_price=config.output_price,
        )

    return None


async def query_models(
    models: list[BaseModel],
    message: str,
    system_prompt: str = None,
) -> list[ChatResponse]:
    """å¹¶å‘æŸ¥è¯¢å¤šä¸ªæ¨¡å‹"""

    # æ„å»ºæ¶ˆæ¯
    messages = []
    if system_prompt:
        messages.append(ChatMessage(role="system", content=system_prompt))
    messages.append(ChatMessage(role="user", content=message))

    # å¹¶å‘è°ƒç”¨æ‰€æœ‰æ¨¡å‹
    tasks = [model.chat(messages) for model in models]
    responses = await asyncio.gather(*tasks, return_exceptions=True)

    # å¤„ç†å¼‚å¸¸
    results = []
    for i, response in enumerate(responses):
        if isinstance(response, Exception):
            results.append(
                ChatResponse(
                    content="",
                    model=models[i].name,
                    provider=models[i].provider,
                    error=str(response),
                )
            )
        else:
            results.append(response)

    return results


async def interactive_mode(models: list[BaseModel]):
    """äº¤äº’æ¨¡å¼"""
    print_welcome()

    # æ˜¾ç¤ºå¯ç”¨æ¨¡å‹
    model_names = [m.name for m in models]
    print_info(f"å·²åŠ è½½ {len(models)} ä¸ªæ¨¡å‹: {', '.join(model_names)}")
    print()

    system_prompt = "ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„åŠ©æ‰‹ï¼Œå›ç­”ç®€æ´æ¸…æ™°ã€‚"

    while True:
        # è·å–ç”¨æˆ·è¾“å…¥
        user_input = get_user_input()

        if not user_input:
            continue

        if user_input.lower() in ("quit", "exit", "q"):
            print_goodbye()
            break

        if user_input.lower() == "help":
            print_info("è¾“å…¥é—®é¢˜è¿›è¡Œå¤šæ¨¡å‹å¯¹æ¯”ï¼Œè¾“å…¥ 'quit' é€€å‡º")
            continue

        # æŸ¥è¯¢æ‰€æœ‰æ¨¡å‹
        print_user_input(user_input)
        print_info("æ­£åœ¨æŸ¥è¯¢å¤šä¸ªæ¨¡å‹...\n")

        responses = await query_models(models, user_input, system_prompt)

        # æ˜¾ç¤ºå“åº”
        print_responses(responses)

        # æ˜¾ç¤ºå¯¹æ¯”è¡¨æ ¼
        print_comparison_table(responses)
        print()


async def single_query_mode(models: list[BaseModel], query: str):
    """å•æ¬¡æŸ¥è¯¢æ¨¡å¼"""
    print_user_input(query)
    print_info("æ­£åœ¨æŸ¥è¯¢å¤šä¸ªæ¨¡å‹...\n")

    responses = await query_models(models, query)

    print_responses(responses)
    print_comparison_table(responses)


def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description="å¤šæ¨¡å‹å¯¹æ¯”èŠå¤©åº”ç”¨",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  python main.py                                    # äº¤äº’æ¨¡å¼
  python main.py --query "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ"         # å•æ¬¡æŸ¥è¯¢
  python main.py --models gpt-4 claude-3-sonnet    # æŒ‡å®šæ¨¡å‹
  python main.py --list-models                      # åˆ—å‡ºå¯ç”¨æ¨¡å‹
        """,
    )

    parser.add_argument("--query", "-q", type=str, help="å•æ¬¡æŸ¥è¯¢çš„é—®é¢˜")

    parser.add_argument("--models", "-m", nargs="+", help="è¦ä½¿ç”¨çš„æ¨¡å‹ï¼ˆç©ºæ ¼åˆ†éš”ï¼‰")

    parser.add_argument("--list-models", action="store_true", help="åˆ—å‡ºæ‰€æœ‰å¯ç”¨æ¨¡å‹")

    return parser.parse_args()


def list_available_models():
    """åˆ—å‡ºæ‰€æœ‰å¯ç”¨æ¨¡å‹"""
    print("\nğŸ“¦ å¯ç”¨æ¨¡å‹åˆ—è¡¨:\n")

    for key, config in MODELS.items():
        api_key = get_api_key(config.provider)
        status = "âœ“" if api_key or config.provider == "ollama" else "âœ— (éœ€è¦ API Key)"
        print(f"  {key:20s} - {config.name:20s} [{config.provider}] {status}")

    print("\nä½¿ç”¨ --models å‚æ•°æŒ‡å®šè¦ä½¿ç”¨çš„æ¨¡å‹")
    print("ä¾‹å¦‚: python main.py --models gpt-3.5-turbo claude-3-haiku\n")


async def main():
    """ä¸»å‡½æ•°"""
    args = parse_args()

    # åˆ—å‡ºæ¨¡å‹
    if args.list_models:
        list_available_models()
        return

    # ç¡®å®šè¦ä½¿ç”¨çš„æ¨¡å‹
    model_keys = args.models if args.models else get_default_models()

    # åˆ›å»ºæ¨¡å‹å®ä¾‹
    models = []
    for key in model_keys:
        model = create_model(key)
        if model:
            models.append(model)

    if not models:
        print_error("æ²¡æœ‰å¯ç”¨çš„æ¨¡å‹ï¼è¯·æ£€æŸ¥ API Key é…ç½®ã€‚")
        print_info("è¿è¡Œ 'python main.py --list-models' æŸ¥çœ‹å¯ç”¨æ¨¡å‹")
        sys.exit(1)

    # è¿è¡Œ
    if args.query:
        await single_query_mode(models, args.query)
    else:
        await interactive_mode(models)


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print_goodbye()
